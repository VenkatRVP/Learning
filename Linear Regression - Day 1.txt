Linear Regression

	- Finds the relationship between features and a label.

Models with Single feature
	
	- y= mx + c
	- y'= b +w1x1
	   -> b - bias
	   -> w1 - Weight of the parameter
	   -> x1 - feature

Model with multiple feature

	- y'= b +w1x1+w2x2+w3x2+w4x4+w5x5
	- Weights and bias will be updated during training
	
Loss
	- Describes how wrong a model's predictions
	- distance between model's predictions and actual labels

Types of Loss
	- L1 Loss = sum(actual value-predicted value)
	- Mean Absolute Error = 1/N * (sum(actual value-predicted value)
)
	-L2 Loss = sum(actual value-predicted value)^2
	- Mean Squared error (1/N *(Sum(actual value-predicted value)^2))

	- The functional difference between L1 loss and L2 loss (or between MAE and MSE) is squaring. When the difference between the prediction and label is large, squaring makes the loss even larger. When the difference is small (less than 1), squaring makes the loss even smaller.


Outlier

	-An outlier can also refer to how far off a model's predictions are from the real values

	- MSE moves the model more toward the outliers, while MAE doesn't. L2 loss incurs a much higher penalty for an outlier than L1 loss. For example, the following images show a model trained using MAE and a model trained using MSE. The red line represents a fully trained model that will be used to make predictions. The outliers are closer to the model trained with MSE than to the model trained with MAE.


Gradient Descent

	- Iteratively adjust the bias and weights of the feature until it reduce the loss
	- Mean Squared Error as loss function
	- It starts with randomized weights and biases near zero
	- Converge

Model Convergence and loss curves
	- The loss curve shows how the loss changes as the model trains.
	- The loss functions for linear models always produce a convex surface

Hyperparameters

	- Hyperparameters are variables that control different aspects of training.
	- Learning Rate, Batch Size, Epochs
	-  Hyperparameters are values that you control; parameters are values that the model calculates during training.

Learning Rate

	- Learning rate is a floating point number you set that influences how quickly the model converges. If the learning rate is too low, the model can take a long time to converge. However, if the learning rate is too high, the model never converges, but instead bounces around the weights and bias that minimize the loss. The goal is to pick a learning rate that's not too high nor too low so that the model converges quickly.

Batch Size

	- Batch size is a hyperparameter that refers to the number of examples the model processes before updating its weights and bias. 

Two Common Techniques
	- stochastic gradient descent and mini-batch 

stochastic gradient descent:
	- Stochastic gradient descent uses only a single example (a batch size of one) per iteration. Given enough iterations, SGD works but is very noisy. "Noise" refers to variations during training that cause the loss to increase rather than decrease during an iteration. The term "stochastic" indicates that the one example comprising each batch is chosen at random.

Mini-batch stochastic gradient descent 

	- Mini-batch stochastic gradient descent is a compromise between full-batch and SGD. For N
 number of data points, the batch size can be any number greater than 1 and less than N.
	- The model chooses the examples included in each batch at random, averages their gradients, and then updates the weights and bias once per iteration.


Epochs
	- An epoch means that the model has processed every example in the training set once. For example, given a training set with 1,000 examples and a mini-batch size of 100 examples, it will take the model 10 iterations to complete one epoch.

